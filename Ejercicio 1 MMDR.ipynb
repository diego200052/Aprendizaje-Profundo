{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM0t5L/ybFOfSpt2ZTcel/7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"dReXgDTuImea"}},{"cell_type":"markdown","source":["# 1. Red de unidades de umbral lineal\n","Programa y evalúa una red de neuronas con funciones de activación escalón unitario que aproxime\n","la operación XNOR (⊙) dada por la siguiente tabla:\n","\n","| $x_1$ | $x_2$ | $y$\n","| ------------- |:-------------:| -----:|\n","|0 |0 |1|\n","|0 |1 |0|\n","|1 |0 |0|\n","|1 |1 |1|"],"metadata":{"id":"TnhrYM6eIrKw"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"pfgO36qkIt7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Se declara la función de activación escalón unitario:\n","\n","$$\n","\\begin{equation}\n","  \\sigma(z) =\n","    \\begin{cases}\n","      0, & \\text{$z < 0$}\\\\\n","      1, & \\text{$z \\geq 0$}\n","    \\end{cases}  \n","\\end{equation}\n","$$"],"metadata":{"id":"ijmTV9hrIwSo"}},{"cell_type":"code","source":["def escalonUnitario(x):\n","    return np.where(x >= 0, 1, 0)"],"metadata":{"id":"HHy5Hzo8Iyck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Se sabe que el XOR (⊕) es equivalente a $\\neg ((x_1 \\land x_2) \\lor \\neg(x_1 \\lor x_2))$. Entonces como el XNOR es la negación del XOR, negando la fórmula anterior se obtiene: $(x_1 \\land x_2) \\lor \\neg(x_1 \\lor x_2)$.  \n","  \n","Asi, para modelar el XNOR se requiere de un AND, un NOR y un OR. A continuación se muestran los valores de pesos y sesgos para aproximar cada una."],"metadata":{"id":"HegnuFDEI0Nc"}},{"cell_type":"markdown","source":["### AND\n","$W_1^{\\{1\\}} = 1,\\ W_2^{\\{1\\}} = 1$ y $\\ b_1^{\\{1\\}} = -1.5$\n","\n","| $x_1$ | $x_2$ | AND | z = $W_1x_1$ + $W_2x_2$ + $b_1^{\\{1\\}}$ | $\\sigma$(z)\n","| ------------- |:-------------:| -----:|:------------------:|:-----:|\n","|0 |0 |0| -1.5| 0\n","|0 |1 |0| -0.5| 0\n","|1 |0 |0| -0.5| 0\n","|1 |1 |1| 0.5|  1"],"metadata":{"id":"FQT4ruM5I2Ju"}},{"cell_type":"markdown","source":["### NOR\n","$W_3^{\\{1\\}} = -1,\\ W_4^{\\{1\\}} = -1$ y $\\ b_2^{\\{1\\}} = 0.5$\n","\n","| $x_1$ | $x_2$ | NOR | z = $W_3^{\\{1\\}}$$x_1$ + $W_4^{\\{1\\}}$$x_2$ + $b_2^{\\{1\\}}$ | $\\sigma$(z)\n","| ------------- |:-------------:| -----:|:------------------:|:-----:|\n","|0 |0 |1| 0.5| 1\n","|0 |1 |0| -0.5| 0\n","|1 |0 |0| -0.5| 0\n","|1 |1 |0| -1.5| 0"],"metadata":{"id":"tW8PzLgaI3bA"}},{"cell_type":"markdown","source":["### OR\n","$W_1^{\\{2\\}} = 1,\\ W_2^{\\{2\\}} = 1$ y $\\ b_1^{\\{2\\}} = -0.5$\n","\n","| $x_1$ | $x_2$ | OR | z = $W_1^{\\{2\\}}$$x_1$ + $W_2^{\\{2\\}}$$x_2$ + $b_1^{\\{2\\}}$ | $\\sigma$(z)\n","| ------------- |:-------------:| -----:|:------------------:|:-----:|\n","|0 |0 |0| -0.5| 0\n","|0 |1 |1| 0.5| 1\n","|1 |0 |1| 0.5| 1\n","|1 |1 |1| 1.5| 1"],"metadata":{"id":"5R_CVVFJI9zC"}},{"cell_type":"markdown","source":["### Diagrama de la red neuronal\n","A continuación se muestra la conexión neuronal que debe realizarse para modelar el XNOR de acuerdo a la fórmula proposicional obtenida $(x_1 \\land x_2) \\lor \\neg(x_1 \\lor x_2)$, así como los pesos y sesgos descritos.\n","\n","![image.png](https://raw.githubusercontent.com/diego200052/Aprendizaje-Profundo-Tarea01-MMDR/master/images/T01-redneuronal.png)"],"metadata":{"id":"QUqo3NaNJCRJ"}},{"cell_type":"markdown","source":["A continuación se muestran los pesos y sesgos que aproximan AND, NOR (para la primera capa) y OR (para la segunda capa).\n","\n","$$\n","W_{AND} = \\left[\\begin{matrix}\n","        1\\\\\n","        1\\\\\n","        \\end{matrix}\\right]\n","b_{AND} = \\left[\\begin{matrix}\n","        -1.5\\\\\n","        \\end{matrix}\\right]\\\\\n","W_{NOR} = \\left[\\begin{matrix}\n","        -1\\\\\n","        -1\\\\\n","        \\end{matrix}\\right]\n","b_{NOR} = \\left[\\begin{matrix}\n","        0.5\\\\\n","        \\end{matrix}\\right]\\\\\n","W^{\\{1\\}} = \\left[\\begin{matrix}\n","        1 & -1\\\\\n","        1 & -1\\\\\n","        \\end{matrix}\\right]\n","b^{\\{1\\}} = \\left[\\begin{matrix}\n","        -1.5\\\\\n","        0.5\\\\\n","        \\end{matrix}\\right]\\\\\n","$$\n","  \n","  \n","$$\n","W_{OR} = \\left[\\begin{matrix}\n","        1\\\\\n","        1\\\\\n","        \\end{matrix}\\right]\n","b_{NOR} = \\left[\\begin{matrix}\n","        -0.5\\\\\n","        \\end{matrix}\\right]\\\\\n","W^{\\{2\\}} = \\left[\\begin{matrix}\n","        1\\\\\n","        1\\\\\n","        \\end{matrix}\\right]\\\\\n","b^{\\{2\\}} = \\left[\\begin{matrix}\n","        -0.5\n","        \\end{matrix}\\right]\\\\\n","$$"],"metadata":{"id":"WkH6cwweJEvS"}},{"cell_type":"code","source":["# Perceptrón AND\n","WAND = np.array([1, 1])\n","bAND = np.array([-1.5])\n","# Perceptrón NOR\n","WNOR = np.array([-1, -1])\n","bNOR = np.array([0.5])\n","\n","# Construir la matriz de pesos W1\n","W1 = np.concatenate((WAND, WNOR)).reshape(2,2).T\n","b1 = np.concatenate((bAND, bNOR))\n","print(f\"W1 = {W1}\")\n","print(f\"b1 = {b1}\\n\")\n","\n","# Perceptrón OR\n","WOR = np.array([1, 1])\n","bOR = np.array([-0.5])\n","\n","# Construir la matriz de pesos W2\n","W2 = WOR\n","b2 = bOR\n","print(f\"W2 = {W2}\")\n","print(f\"b2 = {b2}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2WtH3HIJHer","executionInfo":{"status":"ok","timestamp":1664251972169,"user_tz":300,"elapsed":29,"user":{"displayName":"Diego Medina","userId":"04705228607623341936"}},"outputId":"4a13521d-e067-4721-ba88-3faa474542e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[ 1 -1]\n"," [ 1 -1]]\n","b1 = [-1.5  0.5]\n","\n","W2 = [1 1]\n","b2 = [-0.5]\n"]}]},{"cell_type":"markdown","source":["La suma pesada se puede escribir como la suma de la multiplicación de pesos y entradas, más el sesgo: $z = W_1 \\cdot x_1 + W_2 \\cdot x_2 + W_3 \\cdot x_3 + \\ldots + W_m \\cdot x_m + b$. De manera matricial/vectorial se escribe como : $z = W^{T} x + b$."],"metadata":{"id":"BkMX1RM8JOwf"}},{"cell_type":"code","source":["def sumaPesada(W, x, b):\n","    return W.T @ x + b"],"metadata":{"id":"RTXJ41H2JQRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finalmente, se realiza Feed-forward de manera que la primer capa multiplica las entradas $x$ con $W1$, más el sesgo, y luego pasa por la función de activación escalon unitario para obtener la salida de esa capa. Después para la segunda capa se multiplica $a1$ (la salida de la primer capa) con $W2$, más el sesgo, y se pasa por la función de activación, lo cual es la salida de la red."],"metadata":{"id":"Ehzg1jRBJTX1"}},{"cell_type":"code","source":["def XNOR(x):\n","    z1 = sumaPesada(W1, x, b1)\n","    a1 = escalonUnitario(z1)\n","    z2 = sumaPesada(W2, a1, b2) \n","    y_hat = escalonUnitario(z2)\n","    return y_hat"],"metadata":{"id":"4MmQsSKzJRwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Resultados\n","\n","A continuación se muestran los resultados ante las 4 posibles combinaciones de entradas para la XNOR y sus respectivas salidas."],"metadata":{"id":"iNu3fCkSJV8_"}},{"cell_type":"code","source":["x = np.array([0, 0])\n","print(f\"XNOR: {x} = {XNOR(x)}\")\n","\n","x = np.array([0, 1])\n","print(f\"XNOR: {x} = {XNOR(x)}\")\n","\n","x = np.array([1, 0])\n","print(f\"XNOR: {x} = {XNOR(x)}\")\n","\n","x = np.array([1, 1])\n","print(f\"XNOR: {x} = {XNOR(x)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JPpUrMEJXMt","executionInfo":{"status":"ok","timestamp":1664252065827,"user_tz":300,"elapsed":207,"user":{"displayName":"Diego Medina","userId":"04705228607623341936"}},"outputId":"240541f1-60cb-4ddf-bfa1-64fc4af469a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["XNOR: [0 0] = [1]\n","XNOR: [0 1] = [0]\n","XNOR: [1 0] = [0]\n","XNOR: [1 1] = [1]\n"]}]},{"cell_type":"markdown","source":["**Conclusión:** Un solo perceptrón no es capaz de modelar funciones como la XNOR, debido a que no es linealmente separable. En este caso para solucionarlo, se emplearon tres perceptrones, dos en la primer capa y uno en la segunda capa, que combinan funciones linealmente separables como el AND y el OR para modelar funciones más complejas.\n","  \n","La función de activación de escalón unitario fue útil para tratar con valores discretos binarios. Sin embargo, en retropropagación no sería útil ya que su derivada puede resultar en 0."],"metadata":{"id":"dqD76xD5KG-P"}}]}